import os
import faiss
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Load a lightweight, effective embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

import os
import faiss
import pickle
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter

BASE_DIR = os.path.dirname(os.path.abspath(__file__))


def pdf_to_faiss(pdf_path, member_id):
    """
    Generate FAISS index from a PDF file using local SentenceTransformer embeddings.
    FAISS index is saved in: faiss/{member_id}/index.faiss
    Chunks are saved in: faiss/{member_id}/chunks.pkl
    """
    member_id = 1
    faiss_folder = os.path.join("faiss", str(member_id))
    os.makedirs(faiss_folder, exist_ok=True)
    pdf_path = "../data/attachments/member 1/pdf/doctor_note_noisy1.pdf"

    # Extract text from PDF
    reader = PdfReader(pdf_path)
    full_text = ""
    for page in reader.pages:
        full_text += (page.extract_text() or "") + " "
    full_text = full_text.strip()

    if not full_text:
        raise ValueError("No text found in PDF to index!")

    # Split text into chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, chunk_overlap=200, length_function=len
    )
    chunks = splitter.split_text(full_text)

    # Generate embeddings for each chunk
    embeddings = embedding_model.encode(
        chunks, convert_to_numpy=True, show_progress_bar=True
    )
    embeddings = embeddings.astype("float32")

    # Build FAISS index
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    # Save FAISS index
    faiss_path = os.path.join(faiss_folder, "index.faiss")
    faiss.write_index(index, faiss_path)

    # Save chunks so they can be retrieved later
    chunks_path = os.path.join(faiss_folder, "chunks.pkl")
    with open(chunks_path, "wb") as f:
        pickle.dump(chunks, f)

    return {"faiss_path": faiss_path, "chunks_path": chunks_path, "num_chunks": len(chunks)}


import os
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle

# Load the embedding model once (all-MiniLM-L6-v2 is lightweight and effective)
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')


def answer_question(query, member_id, top_k=3):
    """
    Given a question and member_id, query the FAISS index and return relevant text.

    Args:
        query (str): Doctor's question
        member_id (str/int): ID used for FAISS index folder
        top_k (int): Number of top results to retrieve

    Returns:
        str: Concatenated top_k chunks as answer
    """
    # Build absolute paths
    faiss_folder = os.path.join(BASE_DIR, "faiss", str(member_id))
    index_path = os.path.join(faiss_folder, "index.faiss")
    chunks_path = os.path.join(faiss_folder, "chunks.pkl")

    print("index path is", index_path)
    print("chunkks paths is", chunks_path)

    if not os.path.exists(index_path) or not os.path.exists(chunks_path):
        return "FAISS index or chunks not found. Please generate the index first."

    # Load FAISS index
    index = faiss.read_index(index_path)

    # Load text chunks
    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)  # list of text chunks

    if len(chunks) == 0:
        return "No text chunks found in the index."

    # Embed the query
    query_emb = embedding_model.encode([query], convert_to_numpy=True).astype("float32")

    # Search FAISS
    D, I = index.search(query_emb, top_k)  # I: indices of top chunks

    # Retrieve the relevant chunks safely
    relevant_texts = [chunks[i] for i in I[0] if i < len(chunks)]

    if not relevant_texts:
        return "No relevant text found for the query."

    return "\n\n".join(relevant_texts)
